{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image_Scramble.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xGNn8cTt9tTv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606350651958,"user_tz":300,"elapsed":412,"user":{"displayName":"Jianxin Wang","photoUrl":"","userId":"11142546068393513040"}},"outputId":"0706942e-00df-476d-d0ee-fcc87f2fdfe4"},"source":["%cd /content/drive/My Drive/Human_AI/Texture-Synthesis-Using-Convolutional-Neural-Networks"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Human_AI/Texture-Synthesis-Using-Convolutional-Neural-Networks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3K5-mqn8woD","executionInfo":{"status":"ok","timestamp":1606350653693,"user_tz":300,"elapsed":675,"user":{"displayName":"Jianxin Wang","photoUrl":"","userId":"11142546068393513040"}},"outputId":"d2399708-8a47-45fa-b93f-42b8f5309221"},"source":["%tensorflow_version 1.15"],"execution_count":null,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.15`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gQWjfWIo9zBC"},"source":["from tensorflow_vgg import vgg16_avg_pool\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import helper\n","import tf_helper\n","import tensorflow as tf\n","import os\n","\n","def loss_function(m, texture_op, noise_layers):\n","    loss = tf.constant(0, dtype=tf.float32, name=\"Loss\")\n","\n","    for i in range(len(m)):\n","        texture_filters = np.squeeze(texture_op[m[i][0]], 0)\n","        texture_filters = np.reshape(texture_filters, newshape=(texture_filters.shape[0] * texture_filters.shape[1], texture_filters.shape[2]))\n","        gram_matrix_texture = np.matmul(texture_filters.T, texture_filters)\n","\n","        noise_filters = tf.squeeze(noise_layers[m[i][0]], 0)\n","        noise_filters = tf.reshape(noise_filters, shape=(noise_filters.shape[0] * noise_filters.shape[1], noise_filters.shape[2]))\n","        gram_matrix_noise = tf.matmul(tf.transpose(noise_filters), noise_filters)\n","\n","        denominator = (4 * tf.convert_to_tensor(texture_filters.shape[1], dtype=tf.float32) * tf.convert_to_tensor(texture_filters.shape[0], dtype=tf.float32))\n","\n","        loss += m[i][1] * (tf.reduce_sum(tf.square(tf.subtract(gram_matrix_texture, gram_matrix_noise))) / tf.cast(denominator, tf.float32))\n","    \n","    return loss\n","\n","def run_texture_synthesis(input_filename, m, eps, op_dir, initial_filename, final_filename):\n","    i_w = 256   # width of input image(original image will be scaled down to this width), width of generated image \n","    i_h = 256   # height of input image(original image will be scaled down to this height), height of generated image\n","    \n","    texture_array = helper.resize_and_rescale_img(input_filename, i_w, i_h)\n","    texture_outputs = tf_helper.compute_tf_output(texture_array)\n","    \n","    tf.reset_default_graph()\n","    vgg = vgg16_avg_pool.Vgg16()\n","\n","    random_ = tf.random_uniform(shape=texture_array.shape, minval=0, maxval=0.2)\n","    input_noise = tf.Variable(initial_value=random_, name='input_noise', dtype=tf.float32)\n","\n","    vgg.build(input_noise)\n","\n","    noise_layers_list = dict({0: vgg.conv1_1, 1: vgg.conv1_2, 2: vgg.pool1, 3: vgg.conv2_1, 4: vgg.conv2_2, 5: vgg.pool2, 6: vgg.conv3_1, 7: vgg.conv3_2, \n","                   8: vgg.conv3_3, 9: vgg.pool3, 10: vgg.conv4_1, 11: vgg.conv4_2, 12: vgg.conv4_3, 13: vgg.pool4, 14: vgg.conv5_1, 15: vgg.conv5_2, \n","                   16: vgg.conv5_3, 17: vgg.pool5 })\n","\n","    loss = loss_function(m, texture_outputs, noise_layers_list)\n","    optimizer = tf.train.AdamOptimizer().minimize(loss)\n","\n","\n","    epochs = eps\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","        print(tf.trainable_variables())\n","        init_noise = sess.run(input_noise)\n","        for i in range(epochs):\n","            _, s_loss = sess.run([optimizer, loss])\n","            if (i+1) % 1000 == 0:\n","                print(\"Epoch: {}/{}\".format(i+1, epochs), \" Loss: \", s_loss)\n","        final_noise = sess.run(input_noise)\n","    \n","    initial_noise = helper.post_process_and_display(init_noise, op_dir, initial_filename, save_file=False)\n","    final_noise_ = helper.post_process_and_display(final_noise, op_dir, final_filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tq9xk-uv-AP8"},"source":["m = [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]\n","eps = 500\n","dir = \n","for image in sorted(os.listdir(dir)):\n","    ip_f = dir + '/' + image\n","    output_dir = \"./Output/\"\n","    noise_fn = image + '_noise'\n","    final_fn = image\n","    run_texture_synthesis(ip_f, m, eps, output_dir, noise_fn, final_fn) "],"execution_count":null,"outputs":[]}]}